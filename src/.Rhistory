source_python("C:\\Users\\danie\\OneDrive\\Desktop\\MURPH\\Code\\Optimized\\FCM_Euclidean.py")
conda_create("test_R")
conda_install("test_R", "numpy")
conda_install("test_R", "Pillow")
conda_install("test_R", "numba")
use_condaenv("test_R")
use_condaenv("test_R", required = T)
conda_list()
use_condaenv("test_R", required = T)
conda_list()
use_python()
use_python("C:\Python311\python.exe", required = T)
use_python("C:\\Python311\\python.exe", required = T)
knitr::opts_chunk$set(echo = TRUE)
library(reticulate)
use_python("C:\\Python311\\python.exe", required = T)
use_python("C:\\Python311\\python.exe", required = T)
source_python("C:\\Users\\danie\\OneDrive\\Desktop\\MURPH\\Code\\Optimized\\FCM_Euclidean.py")
source_python("C:\\Users\\danie\\OneDrive\\Desktop\\MURPH\\Code\\Optimized\\FCM_Euclidean.py")
source_python("C:\\Users\\danie\\OneDrive\\Desktop\\MURPH\\Code\\Optimized\\FCM_Euclidean.py")
source_python("C:\\Users\\danie\\OneDrive\\Desktop\\MURPH\\Code\\Optimized\\FCM_Euclidean.py")
source_python("C:\\Users\\danie\\OneDrive\\Desktop\\MURPH\\Code\\Optimized\\FCM_Euclidean.py")
py_install("numba")
source_python("C:\\Users\\danie\\OneDrive\\Desktop\\MURPH\\Code\\Optimized\\FCM_Euclidean.py")
use_condaenv("base")
use_condaenv("base", required = T)
source_python("C:\\Users\\danie\\OneDrive\\Desktop\\MURPH\\Code\\Optimized\\FCM_Euclidean.py")
knitr::opts_chunk$set(echo = TRUE)
library(reticulate)
use_condaenv("base", required = T)
source_python("C:\\Users\\danie\\OneDrive\\Desktop\\MURPH\\Code\\Optimized\\FCM_Euclidean.py")
source_python("C:\\Users\\danie\\OneDrive\\Desktop\\MURPH\\Code\\Optimized\\FCM_Euclidean.py", python_path = "C:\\Users\\danie\\miniconda3\\python.exe")
use_python("C:\\Users\\danie\\miniconda3\\python.exe", required = T)
knitr::opts_chunk$set(echo = TRUE)
library(reticulate)
use_python("C:\\Users\\danie\\miniconda3\\python.exe", required = T)
source_python("C:\\Users\\danie\\OneDrive\\Desktop\\MURPH\\Code\\Optimized\\FCM_Euclidean.py")
use_python("C:\\Users\\danie\\miniconda3\\python.exe", required = T)
source_python("C:\\Users\\danie\\OneDrive\\Desktop\\MURPH\\Code\\Optimized\\FCM_Euclidean.py")
knitr::opts_chunk$set(echo = TRUE)
use_python("C:\\Users\\danie\\miniconda3\\python.exe", required = T)
library(reticulate)
use_python("C:\\Users\\danie\\miniconda3\\python.exe", required = T)
library(reticulate)
use_python("C:\\Users\\danie\\miniconda3\\python.exe", required = T)
use_python("C:\\Users\\danie\\miniconda3\\python.exe", required = T)
source_python("C:\\Users\\danie\\OneDrive\\Desktop\\MURPH\\Code\\Optimized\\FCM_Euclidean.py")
knitr::opts_chunk$set(echo = TRUE)
library(reticulate)
use_python("C:\\Users\\danie\\miniconda3\\python.exe", required = T)
source_python("C:\\Users\\danie\\OneDrive\\Desktop\\MURPH\\Code\\Optimized\\FCM_Euclidean.py")
library(reticulate)
use_python("C:\\Users\\danie\\miniconda3\\python.exe", required = T)
source_python("C:\\Users\\danie\\OneDrive\\Desktop\\MURPH\\Code\\Optimized\\FCM_Euclidean.py")
FCM_Euclidean("C:\\Users\\danie\\OneDrive\\Desktop\\download.jpg")
library(reticulate)
use_python("C:\\Users\\danie\\miniconda3\\python.exe", required = T)
source_python("C:\\Users\\danie\\OneDrive\\Desktop\\MURPH\\Code\\Optimized\\FCM_Euclidean.py")
FCM_Euclidean("C:\\Users\\danie\\OneDrive\\Desktop\\download.jpg")
library(reticulate)
use_python("C:\\Users\\danie\\miniconda3\\python.exe", required = T)
source_python("C:\\Users\\danie\\OneDrive\\Desktop\\MURPH\\Code\\Optimized\\FCM_Euclidean.py")
FCM_Euclidean("C:\\Users\\danie\\OneDrive\\Desktop\\download.jpg")
library(reticulate)
use_python("C:\\Users\\danie\\miniconda3\\python.exe", required = T)
source_python("C:\\Users\\danie\\OneDrive\\Desktop\\MURPH\\Code\\Optimized\\FCM_Euclidean.py")
FCM_Euclidean("C:\\Users\\danie\\OneDrive\\Desktop\\download.jpg")
getwd()
library(reticulate)
use_python("C:\\Users\\danie\\miniconda3\\python.exe", required = T)
source_python("C:\\Users\\danie\\OneDrive\\Desktop\\MURPH\\Code\\Optimized\\FCM_Euclidean.py")
FCM_Euclidean("C:\\Users\\danie\\OneDrive\\Desktop\\download.jpg")
library(reticulate)
use_python("C:\\Users\\danie\\miniconda3\\python.exe", required = T)
source_python("C:\\Users\\danie\\OneDrive\\Desktop\\MURPH\\Code\\Optimized\\FCM_Euclidean.py")
FCM_Euclidean("C:\\Users\\danie\\OneDrive\\Desktop\\download.jpg","C:\\Users\\danie\\OneDrive\\Desktop\\")
library(reticulate)
use_python("C:\\Users\\danie\\miniconda3\\python.exe", required = T)
source_python("C:\\Users\\danie\\OneDrive\\Desktop\\MURPH\\Code\\Optimized\\FCM_Euclidean.py")
library(reticulate)
use_python("C:\\Users\\danie\\miniconda3\\python.exe", required = T)
source_python("C:\\Users\\danie\\OneDrive\\Desktop\\MURPH\\Code\\Optimized\\FCM.py")
FCM_Mahalnobis("C:\\Users\\danie\\OneDrive\\Desktop\\download.jpg","C:\\Users\\danie\\OneDrive\\Desktop\\")
getwd()
setwd("C:\\Users\\danie\\OneDrive\\Desktop\\Courses\\Data 410\\Project\\Code\\src")
knitr::opts_chunk$set(echo = TRUE)
read.csv("../data/AmItheAsshole.csv")
# Loading in data
```{r loading in data}
AmItheAsshole <- read.csv("../data/AmItheAsshole.csv")
askreddit <- read.csv("../data/askreddit.csv")
food <- read.csv("../data/food.csv")
funny <- read.csv("../data/funny.csv")
gaming <- read.csv("../data/gaming.csv")
interestingasfuck <- read.csv("../data/interestingasfuck.csv")
news <- read.csv("../data/news.csv")
todayilearned <- read.csv("../data/todayilearned.csv")
todayilearned
subset(AmItheAsshole, Sarcastic == 1)
# Loading in data
AmItheAsshole <- read.csv("../data/AmItheAsshole.csv")
askreddit <- read.csv("../data/askreddit.csv")
food <- read.csv("../data/food.csv")
funny <- read.csv("../data/funny.csv")
gaming <- read.csv("../data/gaming.csv")
interestingasfuck <- read.csv("../data/interestingasfuck.csv")
news <- read.csv("../data/news.csv")
todayilearned <- read.csv("../data/todayilearned.csv")
datasets <- c(AmItheAsshole, askreddit, food, funny, gaming, interestingasfuck, news, todayilearned)
x <- c()
for(dataset in datasets){
x <- c(x,subset(dataset, Sarcastic == 1))
}
# Loading in data
AmItheAsshole <- read.csv("../data/AmItheAsshole.csv")
askreddit <- read.csv("../data/askreddit.csv")
food <- read.csv("../data/food.csv")
funny <- read.csv("../data/funny.csv")
gaming <- read.csv("../data/gaming.csv")
interestingasfuck <- read.csv("../data/interestingasfuck.csv")
news <- read.csv("../data/news.csv")
todayilearned <- read.csv("../data/todayilearned.csv")
datasets <- list(AmItheAsshole, askreddit, food, funny, gaming, interestingasfuck, news, todayilearned)
datasets
x <- c()
for(dataset in datasets){
x <- c(x,subset(dataset, Sarcastic == 1))
}
x
sarcastic <- list()
for(dataset in datasets){
sarcastic <- list(sarcastic,subset(dataset, Sarcastic == 1))
}
sarcastic
sarcastic[[1]]
sarcastic[[2]]
sarcastic[[3]]
sarcastic[1
sarcastic[1]
sarcastic[1]
sarcastic[2]
subset(AmItheAsshole, sarcastic==1)
subset(AmItheAsshole, Sarcastic==1)
# Loading in data
AmItheAsshole <- read.csv("../data/AmItheAsshole.csv")
askreddit <- read.csv("../data/askreddit.csv")
food <- read.csv("../data/food.csv")
funny <- read.csv("../data/funny.csv")
gaming <- read.csv("../data/gaming.csv")
interestingasfuck <- read.csv("../data/interestingasfuck.csv")
news <- read.csv("../data/news.csv")
todayilearned <- read.csv("../data/todayilearned.csv")
datasets <- list(AmItheAsshole, askreddit, food, funny, gaming, interestingasfuck, news, todayilearned)
lapply(df_list, subset, Sarcastic==1)
lapply(datasets, subset, Sarcastic==1)
do.call(rbind,lapply(datasets, subset, Sarcastic==1))
non_sarcastic <- do.call(rbind,lapply(datasets, subset, Sarcastic==0))
# 1:2
sarcastic
# 1:2
sarcastic[1]
sarcastic <- do.call(rbind,lapply(datasets, subset, Sarcastic==1))
non_sarcastic <- do.call(rbind,lapply(datasets, subset, Sarcastic==0))
# 1:2
sarcastic
# 1:2
class(sarcastic)
# 1:2
sarcastic
# 1:2
nrow(sarcastic)
# 1:2
sarcastic[1,]
# 1:2
sample(sarcastic[,],2)
library(dplyr)
?sample_n
# 1:2
sample_n(sarcasti,2)
# 1:2
sample_n(sarcastic,2)
nrow(sarcastic)
# 1:2
sample_n(non_sarcastic,164)
164*2
4 * 164
9*164
# 1:2
data_1to2 <- rbind(sample_n(non_sarcastic,164),sarcastic)
data_1to2
nrow(data_1to2)
# 1:3
data_1to3 <- rbind(sample_n(non_sarcastic,328),sarcastic)
data_1to3
nrow(data_1to3)
164*4
164*3
# 1:5
data_1to5 <- rbind(sample_n(non_sarcastic,656),sarcastic)
nrow(data_1to5)
164*5
# 1:10
data_1to10 <- rbind(sample_n(non_sarcastic,1476),sarcastic)
nrow(data_1to10)
data_1to10
data_1to2$Body <- gsub("/s$", "", data_1to2$Body)
library(dplyr)
# Generate datasets with differnet ratios of sarcastic:normal
# 1:2
data_1to2 <- rbind(sample_n(non_sarcastic,164),sarcastic)
# 1:3
data_1to3 <- rbind(sample_n(non_sarcastic,328),sarcastic)
# 1:5
data_1to5 <- rbind(sample_n(non_sarcastic,656),sarcastic)
# 1:10
data_1to10 <- rbind(sample_n(non_sarcastic,1476),sarcastic)
data_1to2
data_1to2$Body <- gsub("/s$", "", data_1to2$Body)
data_1to2
# Loading in data
AmItheAsshole <- read.csv("../data/AmItheAsshole.csv")
askreddit <- read.csv("../data/askreddit.csv")
food <- read.csv("../data/food.csv")
funny <- read.csv("../data/funny.csv")
gaming <- read.csv("../data/gaming.csv")
interestingasfuck <- read.csv("../data/interestingasfuck.csv")
news <- read.csv("../data/news.csv")
todayilearned <- read.csv("../data/todayilearned.csv")
datasets <- list(AmItheAsshole, askreddit, food, funny, gaming, interestingasfuck, news, todayilearned)
sarcastic <- do.call(rbind,lapply(datasets, subset, Sarcastic==1))
non_sarcastic <- do.call(rbind,lapply(datasets, subset, Sarcastic==0))
library(dplyr)
# Generate datasets with differnet ratios of sarcastic:normal
# 1:2
data_1to2 <- rbind(sample_n(non_sarcastic,164),sarcastic)
# 1:3
data_1to3 <- rbind(sample_n(non_sarcastic,328),sarcastic)
# 1:5
data_1to5 <- rbind(sample_n(non_sarcastic,656),sarcastic)
# 1:10
data_1to10 <- rbind(sample_n(non_sarcastic,1476),sarcastic)
data_1to2$Body <- gsub("/s$", "", data_1to2$Body)
data_1to3$Body <- gsub("/s$", "", data_1to3$Body)
data_1to5$Body <- gsub("/s$", "", data_1to5$Body)
data_1to10$Body <- gsub("/s$", "", data_1to10$Body)
write.csv(data_1to2, "data_1to2.csv", row.names = FALSE)
knitr::opts_chunk$set(echo = TRUE)
# Loading in data
AmItheAsshole <- read.csv("../data/AmItheAsshole.csv")
askreddit <- read.csv("../data/askreddit.csv")
food <- read.csv("../data/food.csv")
funny <- read.csv("../data/funny.csv")
gaming <- read.csv("../data/gaming.csv")
interestingasfuck <- read.csv("../data/interestingasfuck.csv")
news <- read.csv("../data/news.csv")
todayilearned <- read.csv("../data/todayilearned.csv")
datasets <- list(AmItheAsshole, askreddit, food, funny, gaming, interestingasfuck, news, todayilearned)
sarcastic <- do.call(rbind,lapply(datasets, subset, Sarcastic==1))
non_sarcastic <- do.call(rbind,lapply(datasets, subset, Sarcastic==0))
library(dplyr)
# Generate datasets with differnet ratios of sarcastic:normal
# 1:2
data_1to2 <- rbind(sample_n(non_sarcastic,164),sarcastic)
# 1:3
data_1to3 <- rbind(sample_n(non_sarcastic,328),sarcastic)
# 1:5
data_1to5 <- rbind(sample_n(non_sarcastic,656),sarcastic)
# 1:10
data_1to10 <- rbind(sample_n(non_sarcastic,1476),sarcastic)
data_1to2$Body <- gsub("/s$", "", data_1to2$Body)
data_1to3$Body <- gsub("/s$", "", data_1to3$Body)
data_1to5$Body <- gsub("/s$", "", data_1to5$Body)
data_1to10$Body <- gsub("/s$", "", data_1to10$Body)
write.csv(data_1to2, "data_1to2.csv", row.names = FALSE)
write.csv(data_1to3, "data_1to3.csv", row.names = FALSE)
write.csv(data_1to5, "data_1to5.csv", row.names = FALSE)
write.csv(data_1to10, "data_1to10.csv", row.names = FALSE)
knitr::opts_chunk$set(echo = TRUE)
library(tm)
library(MLmetrics)
data_1to2 <- read.csv("C:\\Users\\danie\\OneDrive\\Desktop\\Data-410-Sarcasm-Detection\\src\\data_1to2_vaxxhappenend.csv",header=TRUE)
data_1to2
data_1to2$doc_id <- rownames(data_1to2)
y <- data_1to2$Sarcastic
data_1to2 <- data_1to2[,c("doc_id","Body")]
colnames(data_1to2)[2] <- "text"
data_1to2$text <- rm_non_words(data_1to2$text)
library(tm)
library(MLmetrics)
library(glmnet)
library(qdapRegex)
data_1to2 <- read.csv("C:\\Users\\danie\\OneDrive\\Desktop\\Data-410-Sarcasm-Detection\\src\\data_1to2_vaxxhappenend.csv",header=TRUE)
data_1to2
data_1to2$doc_id <- rownames(data_1to2)
y <- data_1to2$Sarcastic
data_1to2 <- data_1to2[,c("doc_id","Body")]
colnames(data_1to2)[2] <- "text"
data_1to2$text <- rm_non_words(data_1to2$text)
ds <- DataframeSource(data_1to2)
corpus <- Corpus(ds)
inspect(corpus[1:5])
# Clean the Corpus
# Remove punctuation
corpus <- tm_map(corpus, content_transformer(removePunctuation))
# Send everything to lower case
corpus <- tm_map(corpus, content_transformer(tolower))
# Remove stopwords
corpus <- tm_map(corpus, content_transformer(removeWords),
stopwords("english"))
# Remove whitespace
corpus <- tm_map(corpus, stripWhitespace)
# Remove numbers
corpus <- tm_map(corpus, content_transformer(removeNumbers))
# Stemming
corpus <- tm_map(corpus, stemDocument)
inspect(corpus[1:5])
# Create DTM object
dtm <- DocumentTermMatrix(corpus)
# Weight TF-IDF
tfidf_dtm <- weightTfIdf(dtm)
inspect(tfidf_dtm[,1:3])
tfidf_mat <- as.matrix(tfidf_dtm)
tfidf_vec <- as.vector(tfidf_mat)
tfidf_vec <- tfidf_vec[tfidf_vec > 0]
quantile(tfidf_vec, probs = seq(0.7,0.9,0.05))
# Train model  1.214813
x_data <- as.data.frame(as.matrix(tfidf_dtm[,tfidf_dtm$v >= 1.2172968]))
y_data <- y
data <- cbind(y_data,x_data)
colnames(data)[1] <- "y"
glm.out <- glm(y ~ ., family = binomial, data)
summary(glm.out)
predict.out <- predict(glm.out, type="response")
predict.out[which(predict.out >= 0.5)] <- 1
predict.out[which(predict.out < 0.5)] <- 0
guess <- data.frame(predict.out)
output <- cbind(guess,y)
colnames(output)[2] <- "truth"
output
# Set up test data
tfidf_dtm_test <- tfidf_dtm[test_idx,]
cm <- ConfusionMatrix(output$predict.out, output$truth)
print(cm)
print("Recall: ")
print(cm[4]/(cm[2] + cm[4]))
MLmetrics::
??MLmetrics
cm[1]
print(cm)
cm[2]
cm[3]
cm[4]
cm <- ConfusionMatrix(output$predict.out, output$truth)
print(cm)
print("Recall: ")
print(cm[4]/(cm[2] + cm[4]))
print("Classification Accuracy: ")
(cm[1] + cm[4]) / (cm[1] + cm[2] + cm[3] + cm[4])
train_idx <- sample(1:nrow(data_1to2),floor(.6*(nrow(data_1to10))))
train_idx <- sample(1:nrow(data_1to2),floor(.6*(nrow(data_1to2))))
set.seed(87460945)
train_idx <- sample(1:nrow(data_1to2),floor(.6*(nrow(data_1to2))))
test_idx <- setdiff(1:nrow(data_1to2),train_idx)
data_train <- data_1to2[train_idx,]
data_test <- data_1to2[test_idx,]
y_test <- data_test$Sarcastic
tfidf_dtm_train <- tfidf_dtm[train_idx,]
tfidf_dtm_train <- tfidf_dtm_train[,tfidf_dtm_train$v >= 1.2172968]
inpsect(tfidf_dtm_train)
inspect(tfidf_dtm_train)
set.seed(87460945)
train_idx <- sample(1:nrow(data_1to2),floor(.6*(nrow(data_1to2))))
test_idx <- setdiff(1:nrow(data_1to2),train_idx)
data_train <- data_1to2[train_idx,]
data_test <- data_1to2[test_idx,]
y_test <- data_test$Sarcastic
tfidf_dtm_train <- tfidf_dtm[train_idx,]
tfidf_dtm_train <- tfidf_dtm_train[,tfidf_dtm_train$v >= 1.2172968]
# Train model
x_data <- as.data.frame(as.matrix(tfidf_dtm[,tfidf_dtm$v >= 1.2172968]))
y_data <- y
data <- cbind(y_data,x_data)
colnames(data)[1] <- "y"
glm.out <- glm(y ~ ., family = binomial, data)
summary(glm.out)
data_test
tfidf_dtm[test_idx,]
as.data.frame(tfidf_dtm[test_idx,])
as.data.frame(as.matrix(tfidf_dtm[test_idx,]))
predict.out <- predict(glm.out, newdata = as.data.frame(as.matrix(tfidf_dtm[test_idx,])) ,type="response")
predict.out[which(predict.out >= 0.5)] <- 1
predict.out[which(predict.out < 0.5)] <- 0
guess <- data.frame(predict.out)
output <- cbind(guess,y)
y <- data_1to2[test_idx,c("Sarcastic")]
y
y <- data_1to2[test_idx,]
y
data_1to2 <- read.csv("C:\\Users\\danie\\OneDrive\\Desktop\\Data-410-Sarcasm-Detection\\src\\data_1to2_vaxxhappenend.csv",header=TRUE)
y <- data_1to2[test_idx,]
y
y <- data_1to2[test_idx,c("Sarcastic")]
y
predict.out <- predict(glm.out, newdata = as.data.frame(as.matrix(tfidf_dtm[test_idx,])) ,type="response")
predict.out[which(predict.out >= 0.5)] <- 1
predict.out[which(predict.out < 0.5)] <- 0
guess <- data.frame(predict.out)
data_1to2 <- read.csv("C:\\Users\\danie\\OneDrive\\Desktop\\Data-410-Sarcasm-Detection\\src\\data_1to2_vaxxhappenend.csv",header=TRUE)
y <- data_1to2[test_idx,c("Sarcastic")]
output <- cbind(guess,y)
colnames(output)[2] <- "truth"
output
cm <- ConfusionMatrix(output$predict.out, output$truth)
print(cm)
print("Recall: ")
print(cm[4]/(cm[2] + cm[4]))
print("Classification Accuracy: ")
(cm[1] + cm[4]) / (cm[1] + cm[2] + cm[3] + cm[4])
?MLmetrics::F1_Score()
cm <- ConfusionMatrix(output$predict.out, output$truth)
print(cm)
print("Recall: ")
print(cm[4]/(cm[2] + cm[4]))
print("Classification Accuracy: ")
(cm[1] + cm[4]) / (cm[1] + cm[2] + cm[3] + cm[4])
print("F1 Score")
MLmetrics::F1_Score(output$truth,output$predict.out)
seq(0,1,0.1)
predict.out <- predict(glm.out , type = "response")
f1scores <- c()
for (thresh in seq(0, 1, 0.1)) {
predict.out[which(predict.out >= thresh)] <- 1
predict.out[which(predict.out < thresh)] <- 0
guess <- data.frame(predict.out)
data_1to2 <-
read.csv(
"C:\\Users\\danie\\OneDrive\\Desktop\\Data-410-Sarcasm-Detection\\src\\data_1to2_vaxxhappenend.csv",
header = TRUE
)
y <- data_1to2[test_idx, c("Sarcastic")]
output <- cbind(guess, y)
colnames(output)[2] <- "truth"
f1 <- MLmetrics::F1_Score(output$truth, output$predict.out)
f1scores <- c(f1scores, f1)
}
predict.out <- predict(glm.out , type = "response")
f1scores <- c()
for (thresh in seq(0, 1, 0.1)) {
predict.out[which(predict.out >= thresh)] <- 1
predict.out[which(predict.out < thresh)] <- 0
guess <- data.frame(predict.out)
data_1to2 <-
read.csv(
"C:\\Users\\danie\\OneDrive\\Desktop\\Data-410-Sarcasm-Detection\\src\\data_1to2_vaxxhappenend.csv",
header = TRUE
)
y <- data_1to2[train_idx, c("Sarcastic")]
output <- cbind(guess, y)
colnames(output)[2] <- "truth"
f1 <- MLmetrics::F1_Score(output$truth, output$predict.out)
f1scores <- c(f1scores, f1)
}
predict.out <- predict(glm.out , type = "response")
data_1to2 <-
read.csv(
"C:\\Users\\danie\\OneDrive\\Desktop\\Data-410-Sarcasm-Detection\\src\\data_1to2_vaxxhappenend.csv",
header = TRUE
)
y <- data_1to2[train_idx, c("Sarcastic")]
# Get Train thresholds
predict.out <- predict(glm.out , type = "response")
predict.out
predict.out[which(predict.out >= thresh)] <- 1
predict.out[which(predict.out < thresh)] <- 0
guess <- data.frame(predict.out)
guess
data_1to2 <-
read.csv(
"C:\\Users\\danie\\OneDrive\\Desktop\\Data-410-Sarcasm-Detection\\src\\data_1to2_vaxxhappenend.csv",
header = TRUE
)
y <- data_1to2[train_idx, c("Sarcastic")]
y
output <- cbind(guess, y)
length(y)
guess
# Get Train thresholds
predict.out <- predict(glm.out, newdata = as.data.frame(as.matrix(tfidf_dtm[test_idx,])) ,type="response")
f1scores <- c()
for (thresh in seq(0, 1, 0.1)) {
predict.out[which(predict.out >= thresh)] <- 1
predict.out[which(predict.out < thresh)] <- 0
guess <- data.frame(predict.out)
data_1to2 <- read.csv("C:\\Users\\danie\\OneDrive\\Desktop\\Data-410-Sarcasm-Detection\\src\\data_1to2_vaxxhappenend.csv",header=TRUE)
y <- data_1to2[test_idx,c("Sarcastic")]
output <- cbind(guess,y)
colnames(output)[2] <- "truth"
f1 <- MLmetrics::F1_Score(output$truth, output$predict.out)
f1scores <- c(f1scores, f1)
}
f1scores
thresh <- 0.1
predict.out[which(predict.out >= thresh)] <- 1
predict.out[which(predict.out < thresh)] <- 0
guess <- data.frame(predict.out)
guess
data_1to2 <- read.csv("C:\\Users\\danie\\OneDrive\\Desktop\\Data-410-Sarcasm-Detection\\src\\data_1to2_vaxxhappenend.csv",header=TRUE)
y <- data_1to2[test_idx,c("Sarcastic")]
output <- cbind(guess,y)
output
f1 <- MLmetrics::F1_Score(output$truth, output$predict.out)
getwd()
