vocav = vocab,
term.frequency = term.frequency
)
createJSON(
phi = phi,
theta = theta,
doc.length = doc.length,
vocab = vocab,
term.frequency = term.frequency
)
vocab
doc.length
createJSON(
phi = phi,
theta = theta,
doc.length = doc.length,
vocab = vocab,
term.frequency = term.frequency
)
knitr::opts_chunk$set(echo = TRUE)
data <- read.table("staffing.txt",header = TRUE, sep=",")
data <- read.table("staffing.txt",header = TRUE, sep=",")
```{r}
library(bestglm) #for subset regression
library(bestglm) #for subset regression
library(leaps)
install.packages("leaps")
library(leaps)
library(bestglm) #for subset regression
installed.packages("grpreg")
?bestglm
??bestglm
library(bestglm) #for subset regression
library(bestglm) #for subset regression
installed.packages("grpeg")
library(grpeg)
require(grepeg)
knitr::opts_chunk$set(echo = TRUE)
data <- read.table("staffing.txt",header = TRUE, sep=",")
library(bestglm) #for subset regression
knitr::opts_chunk$set(echo = TRUE)
# Load data
library(text2vec)
library(data.table)
data("movie_review")
setDT(movie_review)
setkey(movie_review, id)
set.seed(2016L)
all_ids = movie_review$id
train_ids = sample(all_ids, 4000)
test_ids = setdiff(all_ids, train_ids)
train = movie_review[J(train_ids)]
test = movie_review[J(test_ids)]
# define preprocessing function and tokenization function
prep_fun = tolower
tok_fun = word_tokenizer
it_train = itoken(train$review,
preprocessor = prep_fun,
tokenizer = tok_fun,
ids = train$id,
progressbar = FALSE)
vocab = create_vocabulary(it_train)
vocab
train_tokens = tok_fun(prep_fun(train$review))
it_train = itoken(train_tokens,
ids = train$id,
# turn off progressbar because it won't look nice in rmd
progressbar = FALSE)
vocab = create_vocabulary(it_train)
vocab
# Create DTM
train_tokens = tok_fun(prep_fun(train$review))
it_train = itoken(train_tokens,
ids = train$id,
# turn off progressbar because it won't look nice in rmd
progressbar = FALSE)
vocab = create_vocabulary(it_train)
vectorizer = vocab_vectorizer(vocab)
t1 = Sys.time()
dtm_train = create_dtm(it_train, vectorizer)
dtm_train
str(dtm_train)
summary(dtm_train)
dim(dtm_train)
library(glmnet)
NFOLDS = 4
glmnet_classifier = cv.glmnet(x = dtm_train, y = train[['sentiment']],
family = 'binomial',
# L1 penalty
alpha = 1,
# interested in the area under ROC curve
type.measure = "auc",
# 5-fold cross-validation
nfolds = NFOLDS,
# high value is less accurate, but has faster training
thresh = 1e-3,
# again lower number of iterations for faster training
maxit = 1e3)
plot(glmnet_classifier)
print(paste("max AUC =", round(max(glmnet_classifier$cvm), 4)))
# Note that most text2vec functions are pipe friendly!
it_test = tok_fun(prep_fun(test$review))
it_test = itoken(it_test, ids = test$id,
# turn off progressbar because it won't look nice in rmd
progressbar = FALSE)
dtm_test = create_dtm(it_test, vectorizer)
preds = predict(glmnet_classifier, dtm_test, type = 'response')[,1]
glmnet:::auc(test$sentiment, preds)
# Create DTM
train_tokens = tok_fun(prep_fun(train$review))
it_train = itoken(train_tokens,
ids = train$id,
# turn off progressbar because it won't look nice in rmd
progressbar = FALSE)
vocab = create_vocabulary(it_train)
vectorizer = vocab_vectorizer(vocab)
t1 = Sys.time()
dtm_train = create_dtm(it_train, vectorizer)
dim(dtm_train)
print(paste("max AUC =", round(max(glmnet_classifier$cvm), 4)))
# Note that most text2vec functions are pipe friendly!
it_test = tok_fun(prep_fun(test$review))
it_test = itoken(it_test, ids = test$id,
# turn off progressbar because it won't look nice in rmd
progressbar = FALSE)
dtm_test = create_dtm(it_test, vectorizer)
preds = predict(glmnet_classifier, dtm_test, type = 'response')[,1]
glmnet:::auc(test$sentiment, preds)
?tok_fun
??tok_fun
# Create DTM
library(tokenizers)
train_tokens = tok_fun(prep_fun(train$review))
it_train = itoken(train_tokens,
ids = train$id,
# turn off progressbar because it won't look nice in rmd
progressbar = FALSE)
vocab = create_vocabulary(it_train)
vectorizer = vocab_vectorizer(vocab)
t1 = Sys.time()
dtm_train = create_dtm(it_train, vectorizer)
dim(dtm_train)
install.packages("languageserver")
knitr::opts_chunk$set(echo = TRUE)
# Load data and split into train/test
library(text2vec)
library(data.table)
data("movie_review")
setDT(movie_review)
setkey(movie_review, id)
set.seed(2016L)
all_ids = movie_review$id
train_ids = sample(all_ids, 4000)
test_ids = setdiff(all_ids, train_ids)
train = movie_review[J(train_ids)]
test = movie_review[J(test_ids)]
# Create DTM
library(languageserver)
train_tokens = tok_fun(prep_fun(train$review))
# Create DTM
library(languageserver)
train_tokens = tok_fun(prep_fun(train$review))
# Create DTM
library(languageserver)
library(tokenizers)
train_tokens = tok_fun(prep_fun(train$review))
# Load data and split into train/test
library(text2vec)
library(data.table)
data("movie_review")
setDT(movie_review)
setkey(movie_review, id)
set.seed(2016L)
all_ids = movie_review$id
train_ids = sample(all_ids, 4000)
test_ids = setdiff(all_ids, train_ids)
train = movie_review[J(train_ids)]
test = movie_review[J(test_ids)]
# Create DTM
train_tokens = tok_fun(prep_fun(train$review))
# Load data and split into train/test
library(text2vec)
library(data.table)
library(tm)
data("movie_review")
setDT(movie_review)
setkey(movie_review, id)
set.seed(2016L)
all_ids = movie_review$id
train_ids = sample(all_ids, 4000)
test_ids = setdiff(all_ids, train_ids)
train = movie_review[J(train_ids)]
test = movie_review[J(test_ids)]
# Create DTM
train_tokens = tok_fun(prep_fun(train$review))
# Load data and split into train/test
library(text2vec)
library(data.table)
data("movie_review")
setDT(movie_review)
setkey(movie_review, id)
set.seed(2016L)
all_ids = movie_review$id
train_ids = sample(all_ids, 4000)
test_ids = setdiff(all_ids, train_ids)
train = movie_review[J(train_ids)]
test = movie_review[J(test_ids)]
# define preprocessing function and tokenization function
prep_fun = tolower
tok_fun = word_tokenizer
it_train = itoken(train$review,
preprocessor = prep_fun,
tokenizer = tok_fun,
ids = train$id,
progressbar = FALSE)
vocab = create_vocabulary(it_train)
# Create DTM
train_tokens = tok_fun(prep_fun(train$review))
it_train = itoken(train_tokens,
ids = train$id,
# turn off progressbar because it won't look nice in rmd
progressbar = FALSE)
vocab = create_vocabulary(it_train)
vectorizer = vocab_vectorizer(vocab)
t1 = Sys.time()
dtm_train = create_dtm(it_train, vectorizer)
dim(dtm_train)
knitr::opts_chunk$set(echo = TRUE)
use_condaenv("base")
library(reticulate)
use_condaenv("base")
use_condaenv("base")
source_python("C:\\Users\\danie\\OneDrive\\Desktop\\MURPH\\Code\\Optimized\\FCM_Euclidean.py")
py_install("Pillow")
source_python("C:\\Users\\danie\\OneDrive\\Desktop\\MURPH\\Code\\Optimized\\FCM_Euclidean.py")
source_python("C:\\Users\\danie\\OneDrive\\Desktop\\MURPH\\Code\\Optimized\\FCM_Euclidean.py")
conda_create("test_R")
conda_install("test_R", "numpy")
conda_install("test_R", "Pillow")
conda_install("test_R", "numba")
use_condaenv("test_R")
use_condaenv("test_R", required = T)
conda_list()
use_condaenv("test_R", required = T)
conda_list()
use_python()
use_python("C:\Python311\python.exe", required = T)
use_python("C:\\Python311\\python.exe", required = T)
knitr::opts_chunk$set(echo = TRUE)
library(reticulate)
use_python("C:\\Python311\\python.exe", required = T)
use_python("C:\\Python311\\python.exe", required = T)
source_python("C:\\Users\\danie\\OneDrive\\Desktop\\MURPH\\Code\\Optimized\\FCM_Euclidean.py")
source_python("C:\\Users\\danie\\OneDrive\\Desktop\\MURPH\\Code\\Optimized\\FCM_Euclidean.py")
source_python("C:\\Users\\danie\\OneDrive\\Desktop\\MURPH\\Code\\Optimized\\FCM_Euclidean.py")
source_python("C:\\Users\\danie\\OneDrive\\Desktop\\MURPH\\Code\\Optimized\\FCM_Euclidean.py")
source_python("C:\\Users\\danie\\OneDrive\\Desktop\\MURPH\\Code\\Optimized\\FCM_Euclidean.py")
py_install("numba")
source_python("C:\\Users\\danie\\OneDrive\\Desktop\\MURPH\\Code\\Optimized\\FCM_Euclidean.py")
use_condaenv("base")
use_condaenv("base", required = T)
source_python("C:\\Users\\danie\\OneDrive\\Desktop\\MURPH\\Code\\Optimized\\FCM_Euclidean.py")
knitr::opts_chunk$set(echo = TRUE)
library(reticulate)
use_condaenv("base", required = T)
source_python("C:\\Users\\danie\\OneDrive\\Desktop\\MURPH\\Code\\Optimized\\FCM_Euclidean.py")
source_python("C:\\Users\\danie\\OneDrive\\Desktop\\MURPH\\Code\\Optimized\\FCM_Euclidean.py", python_path = "C:\\Users\\danie\\miniconda3\\python.exe")
use_python("C:\\Users\\danie\\miniconda3\\python.exe", required = T)
knitr::opts_chunk$set(echo = TRUE)
library(reticulate)
use_python("C:\\Users\\danie\\miniconda3\\python.exe", required = T)
source_python("C:\\Users\\danie\\OneDrive\\Desktop\\MURPH\\Code\\Optimized\\FCM_Euclidean.py")
use_python("C:\\Users\\danie\\miniconda3\\python.exe", required = T)
source_python("C:\\Users\\danie\\OneDrive\\Desktop\\MURPH\\Code\\Optimized\\FCM_Euclidean.py")
knitr::opts_chunk$set(echo = TRUE)
use_python("C:\\Users\\danie\\miniconda3\\python.exe", required = T)
library(reticulate)
use_python("C:\\Users\\danie\\miniconda3\\python.exe", required = T)
library(reticulate)
use_python("C:\\Users\\danie\\miniconda3\\python.exe", required = T)
use_python("C:\\Users\\danie\\miniconda3\\python.exe", required = T)
source_python("C:\\Users\\danie\\OneDrive\\Desktop\\MURPH\\Code\\Optimized\\FCM_Euclidean.py")
knitr::opts_chunk$set(echo = TRUE)
library(reticulate)
use_python("C:\\Users\\danie\\miniconda3\\python.exe", required = T)
source_python("C:\\Users\\danie\\OneDrive\\Desktop\\MURPH\\Code\\Optimized\\FCM_Euclidean.py")
library(reticulate)
use_python("C:\\Users\\danie\\miniconda3\\python.exe", required = T)
source_python("C:\\Users\\danie\\OneDrive\\Desktop\\MURPH\\Code\\Optimized\\FCM_Euclidean.py")
FCM_Euclidean("C:\\Users\\danie\\OneDrive\\Desktop\\download.jpg")
library(reticulate)
use_python("C:\\Users\\danie\\miniconda3\\python.exe", required = T)
source_python("C:\\Users\\danie\\OneDrive\\Desktop\\MURPH\\Code\\Optimized\\FCM_Euclidean.py")
FCM_Euclidean("C:\\Users\\danie\\OneDrive\\Desktop\\download.jpg")
library(reticulate)
use_python("C:\\Users\\danie\\miniconda3\\python.exe", required = T)
source_python("C:\\Users\\danie\\OneDrive\\Desktop\\MURPH\\Code\\Optimized\\FCM_Euclidean.py")
FCM_Euclidean("C:\\Users\\danie\\OneDrive\\Desktop\\download.jpg")
library(reticulate)
use_python("C:\\Users\\danie\\miniconda3\\python.exe", required = T)
source_python("C:\\Users\\danie\\OneDrive\\Desktop\\MURPH\\Code\\Optimized\\FCM_Euclidean.py")
FCM_Euclidean("C:\\Users\\danie\\OneDrive\\Desktop\\download.jpg")
getwd()
library(reticulate)
use_python("C:\\Users\\danie\\miniconda3\\python.exe", required = T)
source_python("C:\\Users\\danie\\OneDrive\\Desktop\\MURPH\\Code\\Optimized\\FCM_Euclidean.py")
FCM_Euclidean("C:\\Users\\danie\\OneDrive\\Desktop\\download.jpg")
library(reticulate)
use_python("C:\\Users\\danie\\miniconda3\\python.exe", required = T)
source_python("C:\\Users\\danie\\OneDrive\\Desktop\\MURPH\\Code\\Optimized\\FCM_Euclidean.py")
FCM_Euclidean("C:\\Users\\danie\\OneDrive\\Desktop\\download.jpg","C:\\Users\\danie\\OneDrive\\Desktop\\")
library(reticulate)
use_python("C:\\Users\\danie\\miniconda3\\python.exe", required = T)
source_python("C:\\Users\\danie\\OneDrive\\Desktop\\MURPH\\Code\\Optimized\\FCM_Euclidean.py")
library(reticulate)
use_python("C:\\Users\\danie\\miniconda3\\python.exe", required = T)
source_python("C:\\Users\\danie\\OneDrive\\Desktop\\MURPH\\Code\\Optimized\\FCM.py")
FCM_Mahalnobis("C:\\Users\\danie\\OneDrive\\Desktop\\download.jpg","C:\\Users\\danie\\OneDrive\\Desktop\\")
getwd()
setwd("C:\\Users\\danie\\OneDrive\\Desktop\\Courses\\Data 410\\Project\\Code\\src")
knitr::opts_chunk$set(echo = TRUE)
read.csv("../data/AmItheAsshole.csv")
# Loading in data
```{r loading in data}
AmItheAsshole <- read.csv("../data/AmItheAsshole.csv")
askreddit <- read.csv("../data/askreddit.csv")
food <- read.csv("../data/food.csv")
funny <- read.csv("../data/funny.csv")
gaming <- read.csv("../data/gaming.csv")
interestingasfuck <- read.csv("../data/interestingasfuck.csv")
news <- read.csv("../data/news.csv")
todayilearned <- read.csv("../data/todayilearned.csv")
todayilearned
subset(AmItheAsshole, Sarcastic == 1)
# Loading in data
AmItheAsshole <- read.csv("../data/AmItheAsshole.csv")
askreddit <- read.csv("../data/askreddit.csv")
food <- read.csv("../data/food.csv")
funny <- read.csv("../data/funny.csv")
gaming <- read.csv("../data/gaming.csv")
interestingasfuck <- read.csv("../data/interestingasfuck.csv")
news <- read.csv("../data/news.csv")
todayilearned <- read.csv("../data/todayilearned.csv")
datasets <- c(AmItheAsshole, askreddit, food, funny, gaming, interestingasfuck, news, todayilearned)
x <- c()
for(dataset in datasets){
x <- c(x,subset(dataset, Sarcastic == 1))
}
# Loading in data
AmItheAsshole <- read.csv("../data/AmItheAsshole.csv")
askreddit <- read.csv("../data/askreddit.csv")
food <- read.csv("../data/food.csv")
funny <- read.csv("../data/funny.csv")
gaming <- read.csv("../data/gaming.csv")
interestingasfuck <- read.csv("../data/interestingasfuck.csv")
news <- read.csv("../data/news.csv")
todayilearned <- read.csv("../data/todayilearned.csv")
datasets <- list(AmItheAsshole, askreddit, food, funny, gaming, interestingasfuck, news, todayilearned)
datasets
x <- c()
for(dataset in datasets){
x <- c(x,subset(dataset, Sarcastic == 1))
}
x
sarcastic <- list()
for(dataset in datasets){
sarcastic <- list(sarcastic,subset(dataset, Sarcastic == 1))
}
sarcastic
sarcastic[[1]]
sarcastic[[2]]
sarcastic[[3]]
sarcastic[1
sarcastic[1]
sarcastic[1]
sarcastic[2]
subset(AmItheAsshole, sarcastic==1)
subset(AmItheAsshole, Sarcastic==1)
# Loading in data
AmItheAsshole <- read.csv("../data/AmItheAsshole.csv")
askreddit <- read.csv("../data/askreddit.csv")
food <- read.csv("../data/food.csv")
funny <- read.csv("../data/funny.csv")
gaming <- read.csv("../data/gaming.csv")
interestingasfuck <- read.csv("../data/interestingasfuck.csv")
news <- read.csv("../data/news.csv")
todayilearned <- read.csv("../data/todayilearned.csv")
datasets <- list(AmItheAsshole, askreddit, food, funny, gaming, interestingasfuck, news, todayilearned)
lapply(df_list, subset, Sarcastic==1)
lapply(datasets, subset, Sarcastic==1)
do.call(rbind,lapply(datasets, subset, Sarcastic==1))
non_sarcastic <- do.call(rbind,lapply(datasets, subset, Sarcastic==0))
# 1:2
sarcastic
# 1:2
sarcastic[1]
sarcastic <- do.call(rbind,lapply(datasets, subset, Sarcastic==1))
non_sarcastic <- do.call(rbind,lapply(datasets, subset, Sarcastic==0))
# 1:2
sarcastic
# 1:2
class(sarcastic)
# 1:2
sarcastic
# 1:2
nrow(sarcastic)
# 1:2
sarcastic[1,]
# 1:2
sample(sarcastic[,],2)
library(dplyr)
?sample_n
# 1:2
sample_n(sarcasti,2)
# 1:2
sample_n(sarcastic,2)
nrow(sarcastic)
# 1:2
sample_n(non_sarcastic,164)
164*2
4 * 164
9*164
# 1:2
data_1to2 <- rbind(sample_n(non_sarcastic,164),sarcastic)
data_1to2
nrow(data_1to2)
# 1:3
data_1to3 <- rbind(sample_n(non_sarcastic,328),sarcastic)
data_1to3
nrow(data_1to3)
164*4
164*3
# 1:5
data_1to5 <- rbind(sample_n(non_sarcastic,656),sarcastic)
nrow(data_1to5)
164*5
# 1:10
data_1to10 <- rbind(sample_n(non_sarcastic,1476),sarcastic)
nrow(data_1to10)
data_1to10
data_1to2$Body <- gsub("/s$", "", data_1to2$Body)
library(dplyr)
# Generate datasets with differnet ratios of sarcastic:normal
# 1:2
data_1to2 <- rbind(sample_n(non_sarcastic,164),sarcastic)
# 1:3
data_1to3 <- rbind(sample_n(non_sarcastic,328),sarcastic)
# 1:5
data_1to5 <- rbind(sample_n(non_sarcastic,656),sarcastic)
# 1:10
data_1to10 <- rbind(sample_n(non_sarcastic,1476),sarcastic)
data_1to2
data_1to2$Body <- gsub("/s$", "", data_1to2$Body)
data_1to2
# Loading in data
AmItheAsshole <- read.csv("../data/AmItheAsshole.csv")
askreddit <- read.csv("../data/askreddit.csv")
food <- read.csv("../data/food.csv")
funny <- read.csv("../data/funny.csv")
gaming <- read.csv("../data/gaming.csv")
interestingasfuck <- read.csv("../data/interestingasfuck.csv")
news <- read.csv("../data/news.csv")
todayilearned <- read.csv("../data/todayilearned.csv")
datasets <- list(AmItheAsshole, askreddit, food, funny, gaming, interestingasfuck, news, todayilearned)
sarcastic <- do.call(rbind,lapply(datasets, subset, Sarcastic==1))
non_sarcastic <- do.call(rbind,lapply(datasets, subset, Sarcastic==0))
library(dplyr)
# Generate datasets with differnet ratios of sarcastic:normal
# 1:2
data_1to2 <- rbind(sample_n(non_sarcastic,164),sarcastic)
# 1:3
data_1to3 <- rbind(sample_n(non_sarcastic,328),sarcastic)
# 1:5
data_1to5 <- rbind(sample_n(non_sarcastic,656),sarcastic)
# 1:10
data_1to10 <- rbind(sample_n(non_sarcastic,1476),sarcastic)
data_1to2$Body <- gsub("/s$", "", data_1to2$Body)
data_1to3$Body <- gsub("/s$", "", data_1to3$Body)
data_1to5$Body <- gsub("/s$", "", data_1to5$Body)
data_1to10$Body <- gsub("/s$", "", data_1to10$Body)
write.csv(data_1to2, "data_1to2.csv", row.names = FALSE)
knitr::opts_chunk$set(echo = TRUE)
# Loading in data
AmItheAsshole <- read.csv("../data/AmItheAsshole.csv")
askreddit <- read.csv("../data/askreddit.csv")
food <- read.csv("../data/food.csv")
funny <- read.csv("../data/funny.csv")
gaming <- read.csv("../data/gaming.csv")
interestingasfuck <- read.csv("../data/interestingasfuck.csv")
news <- read.csv("../data/news.csv")
todayilearned <- read.csv("../data/todayilearned.csv")
datasets <- list(AmItheAsshole, askreddit, food, funny, gaming, interestingasfuck, news, todayilearned)
sarcastic <- do.call(rbind,lapply(datasets, subset, Sarcastic==1))
non_sarcastic <- do.call(rbind,lapply(datasets, subset, Sarcastic==0))
library(dplyr)
# Generate datasets with differnet ratios of sarcastic:normal
# 1:2
data_1to2 <- rbind(sample_n(non_sarcastic,164),sarcastic)
# 1:3
data_1to3 <- rbind(sample_n(non_sarcastic,328),sarcastic)
# 1:5
data_1to5 <- rbind(sample_n(non_sarcastic,656),sarcastic)
# 1:10
data_1to10 <- rbind(sample_n(non_sarcastic,1476),sarcastic)
data_1to2$Body <- gsub("/s$", "", data_1to2$Body)
data_1to3$Body <- gsub("/s$", "", data_1to3$Body)
data_1to5$Body <- gsub("/s$", "", data_1to5$Body)
data_1to10$Body <- gsub("/s$", "", data_1to10$Body)
write.csv(data_1to2, "data_1to2.csv", row.names = FALSE)
write.csv(data_1to3, "data_1to3.csv", row.names = FALSE)
write.csv(data_1to5, "data_1to5.csv", row.names = FALSE)
write.csv(data_1to10, "data_1to10.csv", row.names = FALSE)
